# APIForge

**Enterprise-grade API test case generator from OpenAPI specifications**

APIForge automatically analyzes your OpenAPI/Swagger specifications and generates comprehensive, structured test cases covering positive, negative, and boundary scenarios using Large Language Models (LLMs).

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

## English

## ğŸŒŸ Features

- **Automated Test Case Generation**: Generate comprehensive test cases from OpenAPI/Swagger specifications
- **Multiple LLM Providers**: Support for OpenAI, Custom APIs, and Qwen (with /no_think optimization for 10x speed)
- **Enterprise-Grade Architecture**: Async-first design with robust error handling and retry logic
- **Advanced Test Design Methods**: 
  - Boundary Value Analysis (BVA) - Test edge cases and limits
  - Decision Tables - Cover all logical combinations
  - State Transition - Test API state changes
  - Pairwise Testing - Optimize test combinations
  - Equivalence Partitioning - Group similar test scenarios
- **Enhanced Test Coverage**: Generates 5-20 test cases per endpoint using multiple testing strategies
- **Intelligent Task Scheduling**: 
  - SQLite-based persistent task queue
  - Dynamic worker scaling based on load
  - Progressive scheduling with API pattern matching
  - Hybrid scheduling with real-time optimization
- **Web-based Monitoring Dashboard**: 
  - Real-time progress tracking with WebSocket
  - Session management and history
  - Task retry and error tracking
  - Performance metrics and statistics
  - Export reports (CSV/JSON)
- **Multiple Output Formats**: 
  - JSON with standardized schema
  - CSV for spreadsheet tools and test management systems
  - Intermediate files for debugging
- **Production-Ready Features**:
  - Robust error handling and automatic retries
  - Rate limiting and concurrency control
  - Memory-efficient processing for large APIs
  - Graceful shutdown and task persistence
- **Extensible Provider System**: Easy to add support for new LLM providers
- **Security-First**: Secure handling of API keys and sensitive information
- **Rich CLI Interface**: Beautiful command-line interface with progress indicators

## ğŸš€ Quick Start

### ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/apiforge/apiforge.git
cd apiforge

# Install dependencies
pip install -e .
```

### âš™ï¸ Configuration

1. Copy the environment template:
```bash
cp .env.example .env
```

2. Configure your preferred LLM provider:

**For Qwen (Default - Fast & Free):**
```bash
LLM_PROVIDER=qwen
# No API key required, uses default endpoint
```

**For OpenAI:**
```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-openai-api-key-here
```

**For Custom API:**
```bash
LLM_PROVIDER=custom
CUSTOM_API_KEY=your-api-key
CUSTOM_BASE_URL=https://your-api.com/v1
CUSTOM_MODEL=your-model-name
```

### ğŸ¯ Basic Usage

Generate test cases from an OpenAPI specification:

```bash
# Basic usage (uses default Qwen provider)
python run.py --url https://petstore.swagger.io/v2/swagger.json --output petstore_tests.json

# Generate test cases in CSV format for spreadsheet tools
python run.py --url https://petstore.swagger.io/v2/swagger.json --output petstore_tests.csv

# With intermediate file outputs for debugging
python run.py --url https://api.example.com/spec.json --output tests.json --intermediate

# Using a specific provider
python run.py --url https://api.example.com/spec.json --output tests.json --provider openai

# Run with different execution modes
python run.py --url https://api.example.com/spec.json --output tests.json --mode auto
# Available modes: auto (intelligent progressive), fast (maximum concurrency), 
#                  smart (dynamic scheduling), ai-analysis (AI-powered deep analysis)
```

### ğŸ“Š Monitoring Dashboard

APIForge includes a powerful monitoring dashboard for tracking test generation progress:

```bash
# Start the monitoring dashboard standalone
python run.py dashboard

# Or run generation with automatic monitoring
python run.py --url https://api.example.com/spec.json --output tests.json --monitor
```

Access the dashboard at:
- **Main Dashboard**: http://localhost:9099
- **Real-time Monitor**: http://localhost:9099/monitor
- **Error Logs**: http://localhost:9099/errors
- **Statistics**: http://localhost:9099/statistics

Features:
- Real-time progress tracking with WebSocket updates
- Session history and management
- Task status monitoring with retry capabilities
- Performance metrics and statistics
- Error tracking and analysis
- Export functionality for reports (CSV/JSON)

### ğŸ’» Programmatic Usage

```python
import asyncio
from apiforge.orchestrator import run_generation

async def generate_tests():
    await run_generation(
        url="https://petstore.swagger.io/v2/swagger.json",
        output_path="test_suite.json"
    )

asyncio.run(generate_tests())
```

## ğŸ“– Documentation

### ğŸ’¡ Command Line Interface

```bash
# Generate test cases
python run.py --url <OPENAPI_URL> --output <OUTPUT_FILE> [OPTIONS]

# Show system information
python run.py info 

# Available options:
#   --url, -u          URL of OpenAPI specification (required)
#   --output, -o       Output file path (.json or .csv) (required)
#   --verbose, -v      Enable verbose logging
#   --provider, -p     LLM provider to use (default: qwen)
#   --intermediate     Save intermediate processing files
#   --mode, -m         Execution mode: auto|fast|smart|ai-analysis (default: auto)
#   --monitor, -M      Enable real-time monitoring dashboard
#   --help            Show help message

# Generate CSV template for manual test creation
python generate_csv_template.py --output test_template.csv
```

### âš™ï¸ Configuration Options

All configuration can be set via environment variables in your `.env` file:

```env
# OpenAI Configuration
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-4
OPENAI_MAX_TOKENS=4000
OPENAI_TEMPERATURE=0.1

# Performance Settings
MAX_CONCURRENT_REQUESTS=10
RATE_LIMIT_PER_MINUTE=100

# Output Settings
OUTPUT_FORMAT=json
OUTPUT_INDENT=2
VALIDATE_OUTPUT=true

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=structured
```

### ğŸ’¾ Output Formats

APITestGen supports multiple output formats:

#### JSON Format (Default)

Generates test suites in a standardized JSON format:

```json
{
  "testSuite": {
    "name": "API Test Suite",
    "description": "Automatically generated test suite",
    "baseUrl": "https://api.example.com",
    "testCases": [
      {
        "id": "TC_001",
        "name": "Create user with valid data",
        "description": "Verify successful user creation",
        "priority": "High",
        "category": "positive",
        "tags": ["users", "create"],
        "request": {
          "method": "POST",
          "endpoint": "/v1/users",
          "headers": {"Content-Type": "application/json"},
          "pathParams": {},
          "queryParams": {},
          "body": {"name": "John Doe", "email": "john@example.com"}
        },
        "expectedResponse": {
          "statusCode": 201,
          "headers": {"Content-Type": "application/json"},
          "bodySchema": {"type": "object", "properties": {...}}
        },
        "preconditions": "System is running",
        "postconditions": "User record created"
      }
    ]
  }
}
```

#### CSV Format

Export test cases in CSV format for use with spreadsheet software or test management tools:

```csv
test_id,test_name,endpoint,method,priority,category,path_params,query_params,headers,request_body,expected_status,expected_headers,expected_body,description,preconditions,postconditions,tags
TC_001,Create user with valid data,/v1/users,POST,High,positive,,,"{"Content-Type":"application/json"}","{"name":"John Doe","email":"john@example.com"}",201,"Content-Type=application/json","{"type":"object"}",Verify successful user creation,System is running,User record created,"users,create"
```

Features:
- Compatible with Excel, Google Sheets, and test management tools
- JSON encoding for complex fields
- Preserves all test case information
- Includes metadata as comments

## ğŸ—ï¸ Architecture

APIForge follows a modular, enterprise-grade architecture:

### ğŸ“Š System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“„ SpecLoader  â”‚â”€â”€â”€â”€â–¶â”‚  ğŸ“‹ SpecParser  â”‚â”€â”€â”€â”€â–¶â”‚  ğŸ¤– Generator   â”‚
â”‚  (Async HTTP)   â”‚     â”‚(Pydantic Models)â”‚     â”‚ (LLM Provider)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚     ğŸ­ Orchestrator          â”‚
                  â”‚  (Workflow Coordination)     â”‚
                  â”‚    + Task Scheduling         â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚            â”‚            â”‚
                     â–¼            â–¼            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ğŸ’¾ SQLite â”‚ â”‚ğŸ“Š Web UI â”‚ â”‚ğŸ“ Output â”‚
              â”‚  Queue   â”‚ â”‚ Monitor  â”‚ â”‚ JSON/CSV â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Data Flow Sequence

```
User Input                  Processing                    Output
    â”‚                           â”‚                           â”‚
    â–¼                           â–¼                           â–¼
ğŸŒ API URL â”€â”€â–¶ ğŸ“¥ Load Spec â”€â”€â–¶ ğŸ” Parse â”€â”€â–¶ ğŸ§  Generate â”€â”€â–¶ ğŸ’¾ Save
                    â”‚               â”‚              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                              ğŸ“Š Monitor Progress
```

### ğŸ§© Core Components

- **ğŸ“„ SpecLoader**: Async HTTP client for loading OpenAPI specifications
- **ğŸ“‹ SpecParser**: Pydantic-based parser for extracting endpoint information
- **ğŸ¤– Generator**: Test case generation with LLM provider abstraction
- **ğŸ­ Orchestrator**: Workflow coordination with concurrent processing
- **âš¡ Task Scheduler**: SQLite-based task queue with retry logic
- **ğŸ“Š Web UI**: FastAPI-based dashboard with real-time monitoring
- **ğŸ”Œ Providers**: Pluggable LLM provider system (OpenAI, Qwen, Custom)

### ğŸ“ˆ Performance & Scalability

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Execution Mode Comparison                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Mode     â”‚  Concurrency   â”‚      Performance        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸš€ Fast     â”‚ Max (10-50)    â”‚ 100+ endpoints/min      â”‚
â”‚ ğŸ¯ Smart    â”‚ Dynamic (5-20) â”‚ 50-80 endpoints/min     â”‚
â”‚ ğŸ“Š Auto     â”‚ Progressive    â”‚ 30-60 endpoints/min     â”‚
â”‚ ğŸ§  AI       â”‚ Limited (1-5)  â”‚ 10-20 endpoints/min     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Development

### ğŸ”¨ Setup Development Environment

```bash
# Install development dependencies
pip install -e ".[dev]"

# Run code formatting
black apiforge/
isort apiforge/

# Run type checking
mypy apiforge/

# Run linting
flake8 apiforge/
```

### ğŸ¤– Adding New LLM Providers

1. Create a new provider class inheriting from `LLMProvider`:

```python
from apiforge.providers.base import LLMProvider

class CustomProvider(LLMProvider):
    @property
    def provider_name(self) -> str:
        return "Custom"
    
    async def generate_test_cases_async(self, endpoint: EndpointInfo) -> List[Dict]:
        # Implementation here
        pass
```

2. Register the provider:

```python
from apiforge.generator import TestCaseGenerator

TestCaseGenerator.register_provider("custom", CustomProvider)
```

### ğŸš€ Running Examples

```bash
# Set up environment
cp .env.example .env
# Edit .env with your API key

# Run Petstore example
cd examples
python generate_petstore_tests.py
```

## ğŸ›¡ï¸ Security

- **API Key Management**: Never hardcode API keys; use environment variables
- **Input Validation**: Comprehensive validation of OpenAPI specifications
- **Output Sanitization**: Automatic sanitization of sensitive information in logs
- **Secure Defaults**: Security-first configuration defaults

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### ğŸ”„ Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass and code is formatted
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [Pydantic](https://pydantic.dev/) for data validation
- Uses [httpx](https://www.python-httpx.org/) for async HTTP requests
- CLI powered by [Click](https://click.palletsprojects.com/) and [Rich](https://rich.readthedocs.io/)
- OpenAI integration via the official [OpenAI Python SDK](https://github.com/openai/openai-python)

## ğŸ“ Support

- **Documentation**: See `/docs` directory for detailed documentation
- **CSV Format**: See `/docs/csv_format_design.md` for CSV format specification
- **Issues**: [GitHub Issues](https://github.com/your-org/apiforge/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-org/apiforge/discussions)

---

**APIForge** - Forging intelligent API tests with AI-powered automation ğŸš€

---

## ä¸­æ–‡

## ğŸŒŸ åŠŸèƒ½ç‰¹æ€§

- **è‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ**ï¼šä» OpenAPI/Swagger è§„èŒƒè‡ªåŠ¨ç”Ÿæˆå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹
- **å¤š LLM æä¾›å•†æ”¯æŒ**ï¼šæ”¯æŒ OpenAIã€è‡ªå®šä¹‰ API å’Œåƒé—®ï¼ˆå¸¦ /no_think ä¼˜åŒ–ï¼Œé€Ÿåº¦æå‡ 10 å€ï¼‰
- **ä¼ä¸šçº§æ¶æ„**ï¼š
  - å¼‚æ­¥ä¼˜å…ˆè®¾è®¡ï¼Œå…·æœ‰å¼ºå¤§çš„é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
  - åŸºäº SQLite çš„æŒä¹…åŒ–ä»»åŠ¡é˜Ÿåˆ—
  - ä¼˜é›…çš„å…³é—­å’Œä»»åŠ¡æ¢å¤æœºåˆ¶
- **é«˜çº§æµ‹è¯•è®¾è®¡æ–¹æ³•**ï¼š
  - è¾¹ç•Œå€¼åˆ†æï¼ˆBVAï¼‰- æµ‹è¯•è¾¹ç¼˜æƒ…å†µå’Œæé™å€¼
  - å†³ç­–è¡¨ - è¦†ç›–æ‰€æœ‰é€»è¾‘ç»„åˆ
  - çŠ¶æ€è½¬æ¢ - æµ‹è¯• API çŠ¶æ€å˜åŒ–
  - æˆå¯¹æµ‹è¯• - ä¼˜åŒ–æµ‹è¯•ç»„åˆ
  - ç­‰ä»·ç±»åˆ’åˆ† - åˆ†ç»„ç›¸ä¼¼æµ‹è¯•åœºæ™¯
- **å¢å¼ºçš„æµ‹è¯•è¦†ç›–ç‡**ï¼šæ¯ä¸ªç«¯ç‚¹ä½¿ç”¨å¤šç§æµ‹è¯•ç­–ç•¥ç”Ÿæˆ 5-20 ä¸ªæµ‹è¯•ç”¨ä¾‹
- **æ™ºèƒ½ä»»åŠ¡è°ƒåº¦**ï¼š
  - åŸºäº SQLite çš„æŒä¹…åŒ–ä»»åŠ¡é˜Ÿåˆ—
  - åŸºäºè´Ÿè½½çš„åŠ¨æ€å·¥ä½œå™¨æ‰©å±•
  - å¸¦ API æ¨¡å¼åŒ¹é…çš„æ¸è¿›å¼è°ƒåº¦
  - å®æ—¶ä¼˜åŒ–çš„æ··åˆè°ƒåº¦
- **Web ç›‘æ§ä»ªè¡¨æ¿**ï¼š
  - åŸºäº WebSocket çš„å®æ—¶è¿›åº¦è·Ÿè¸ª
  - ä¼šè¯ç®¡ç†å’Œå†å²è®°å½•
  - ä»»åŠ¡é‡è¯•å’Œé”™è¯¯è·Ÿè¸ª
  - æ€§èƒ½æŒ‡æ ‡å’Œç»Ÿè®¡æ•°æ®
  - å¯¼å‡ºæŠ¥å‘Šï¼ˆCSV/JSONï¼‰
- **å¤šç§è¾“å‡ºæ ¼å¼**ï¼š
  - æ ‡å‡†åŒ–æ¶æ„çš„ JSON
  - é€‚ç”¨äºç”µå­è¡¨æ ¼å·¥å…·å’Œæµ‹è¯•ç®¡ç†ç³»ç»Ÿçš„ CSV
  - ç”¨äºè°ƒè¯•çš„ä¸­é—´æ–‡ä»¶
- **ç”Ÿäº§å°±ç»ªç‰¹æ€§**ï¼š
  - å¼ºå¤§çš„é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨é‡è¯•
  - é€Ÿç‡é™åˆ¶å’Œå¹¶å‘æ§åˆ¶
  - å¤§å‹ API çš„å†…å­˜é«˜æ•ˆå¤„ç†
  - ä¼˜é›…å…³é—­å’Œä»»åŠ¡æŒä¹…åŒ–
- **å¯æ‰©å±•çš„æä¾›å•†ç³»ç»Ÿ**ï¼šè½»æ¾æ·»åŠ å¯¹æ–° LLM æä¾›å•†çš„æ”¯æŒ
- **å®‰å…¨ä¼˜å…ˆ**ï¼šå®‰å…¨å¤„ç† API å¯†é’¥å’Œæ•æ„Ÿä¿¡æ¯
- **ä¸°å¯Œçš„ CLI ç•Œé¢**ï¼šå¸¦æœ‰è¿›åº¦æŒ‡ç¤ºå™¨çš„ç¾è§‚å‘½ä»¤è¡Œç•Œé¢

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/apiforge/apiforge.git
cd apiforge

# å®‰è£…ä¾èµ–
pip install -e .
```

### é…ç½®

1. å¤åˆ¶ç¯å¢ƒæ¨¡æ¿ï¼š
```bash
cp .env.example .env
```

2. é…ç½®æ‚¨å–œæ¬¢çš„ LLM æä¾›å•†ï¼š

**ä½¿ç”¨åƒé—®ï¼ˆé»˜è®¤ - å¿«é€Ÿä¸”å…è´¹ï¼‰ï¼š**
```bash
LLM_PROVIDER=qwen
# æ— éœ€ API å¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤ç«¯ç‚¹
```

**ä½¿ç”¨ OpenAIï¼š**
```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-openai-api-key-here
```

### åŸºæœ¬ç”¨æ³•

ä» OpenAPI è§„èŒƒç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼š

```bash
# åŸºæœ¬ç”¨æ³•ï¼ˆä½¿ç”¨é»˜è®¤åƒé—®æä¾›å•†ï¼‰
python run.py --url https://petstore.swagger.io/v2/swagger.json --output petstore_tests.json

# ç”Ÿæˆ CSV æ ¼å¼çš„æµ‹è¯•ç”¨ä¾‹
python run.py --url https://petstore.swagger.io/v2/swagger.json --output petstore_tests.csv

# ä½¿ç”¨ä¸åŒçš„æ‰§è¡Œæ¨¡å¼
python run.py --url https://api.example.com/spec.json --output tests.json --mode auto
# å¯ç”¨æ¨¡å¼ï¼šautoï¼ˆæ™ºèƒ½æ¸è¿›å¼ï¼‰ã€fastï¼ˆæœ€å¤§å¹¶å‘ï¼‰ã€
#          smartï¼ˆåŠ¨æ€è°ƒåº¦ï¼‰ã€ai-analysisï¼ˆAI é©±åŠ¨çš„æ·±åº¦åˆ†æï¼‰
```

### ç›‘æ§ä»ªè¡¨æ¿

APIForge åŒ…å«å¼ºå¤§çš„ç›‘æ§ä»ªè¡¨æ¿ï¼Œç”¨äºè·Ÿè¸ªæµ‹è¯•ç”Ÿæˆè¿›åº¦ï¼š

```bash
# å¯åŠ¨ç‹¬ç«‹ç›‘æ§ä»ªè¡¨æ¿
python run.py dashboard

# æˆ–è¿è¡Œç”Ÿæˆå¹¶è‡ªåŠ¨ç›‘æ§
python run.py --url https://api.example.com/spec.json --output tests.json --monitor
```

è®¿é—®ä»ªè¡¨æ¿ï¼š
- **ä¸»ä»ªè¡¨æ¿**ï¼šhttp://localhost:9099
- **å®æ—¶ç›‘æ§**ï¼šhttp://localhost:9099/monitor
- **é”™è¯¯æ—¥å¿—**ï¼šhttp://localhost:9099/errors
- **ç»Ÿè®¡æ•°æ®**ï¼šhttp://localhost:9099/statistics

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

**APIForge** - ç”¨ AI é©±åŠ¨çš„è‡ªåŠ¨åŒ–é”»é€ æ™ºèƒ½ API æµ‹è¯• ğŸš€